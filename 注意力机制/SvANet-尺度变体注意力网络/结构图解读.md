# 整体框架

- SvANet 是典型的**深度编码器—解码器（encoder–decoder）骨架**，输入经过一个 stem（下采样一次）后进入 5 个 stage（stage1..stage5），每个 stage 包含若干卷积/瓶颈单元和 **MCBottleneck**（蓝色），在 stage 末端有下采样 Conv3×3↓2；解码端通过 TConv（转置/上采样块）逐级恢复。最后在最顶层接 **ASPP** 提升多尺度感受野并输出预测。图中有三种颜色的“插入模块”：
  - 蓝色：**MCBottleneck / MCAttn**（局部+跨尺度注意，在 encoder 各 stage 中）。
  - 橙色：**Cross-scale guidance（跨尺度引导） + SvAttn（scale-variant attention）**，用来把早期高分辨率特征“引导”到后面的深层，从而弥补下采样造成的信息丢失。
  - 绿色：**AssemFormer（卷积 + Vision Transformer 混合模块）**，负责把局部卷积特征和全局 transformer 特征结合在一起。

一句话理解：整体像 U-Net，但在每个阶段“塞入”了专门为超小目标设计的注意与 transformer 混合模块，且用跨尺度路径把高分辨率信息传回来以保留小目标细节。

![结构图](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251028111538.jpg)

------

# 1) MCBottleneck 和 Monte Carlo Attention（图左上、蓝色部分）

**结构（图示 + 文本）**

- 每个 MCBottleneck 的顺序是：`Conv3×3 -> Conv1×1 -> MCAttn -> Conv1×1 -> AssemFormer`（论文中明确描述）。这是 encoder 里的“压缩点（bottleneck）”——先压通道再扩回去以提取关键特征。
- **MCAttn（Monte Carlo attention）**：不是单一的全局 AvgPool，而是对多种池化大小（论文中用 3×3、2×2、1×1 三种 pooled tensors）做“随机/蒙特卡洛式”加权组合，输出 attention map：
   $$
   A_m(x)=\sum_{i=1}^n P_1(x,i), f(x,i)
   $$
   其中 (f(x,i)) 是不同输出尺寸的 pooling（平均池化），(P_1) 是满足概率约束的随机关联系数（论文要求和为 1、乘积为 0，以保证多样性）。论文实验证明 (1,2,3) 组合效果最好。

**为什么这么做（直观）**

- 传统全局池化把所有空间信息压成一个尺度（容易丢细节），而 MCAttn 在一个阶段内从 *多尺度 pooled* 得到多种粒度的“注意候选图”，并用蒙特卡洛式的随机权重去组合，既能保留局部细节（小目标）又能得到一定程度的全局上下文，增强了对小、稀疏目标（例如视网膜血管、精子等）的可辨识度。论文里多个对比图显示 MCAttn 能突出更多超小目标的细节。

一句话理解：MCBottleneck 是 encoder 的压缩-扩展单元，里面的 MCAttn 用“随机组合不同池化尺度”的注意力，既能看大图也能抓小点，不会把小目标吞掉。

------

# 2) Cross-scale guidance（跨尺度引导，图中橙色箭头）

**公式与实现**

- 目标：把更早阶段（高分辨率）的特征 (x_s) 用一系列带步长（stride）的 3×3 卷积变换后，累加到目标阶段 t：
   $$
   y_t=\sum_{s=1}^{t-1} g(x_s,t)
   $$
   其中$ (g(x_s,t)) $由 (t−s) 个 3×3 stride 卷积组成（把早期大分辨率特征下采样到 t 的分辨率再融合）。论文图里以橙色箭头表示这些从浅层到深层的引导路径。

**直观说明**

- 深层因为多次下采样会丢细节。Cross-scale guidance 把浅层（细节丰富）的信息“先下采样到相同尺度”再融合，而不是把浅层直接上采回深层，这样可以把浅层高分辨率语义用更“兼容”的方式注入深层网络，保留形状/位置线索（对小目标非常关键）。

一句话理解：把早期“高分辨率的细节”转成和深层一样的尺度再加进去，让深层“记得”小目标的存在和位置。

------

# 3) Scale-variant Attention（SvAttn，图中黄块 / 中间顶部）

**公式与目的**

- SvAttn 在 cross-scale 融合的基础上，再做“尺度变异的全局注意”：
   $$
   A_t(x)=\frac{1}{\sigma(x)}\sum_{j=1}^n \sum_{s=1}^{t-1} P_2(x_{s,j}), x_{s,j}
   $$
   这里把来自不同 stage（不同压缩等级）但“相对位置对应”的子区域 $(x_{s,j}) $加权求和，(P_2) 给出跨 stage 的对应权重（满足概率约束）。最终用 (A_t) 乘以 cross-scale 输出 (y_t)（见论文 Eq.(6)），得到更细粒度的特征。

**直观说明**

- SvAttn 的创新点是 **同时在“位置比例一致”的跨层子区域上计算注意力**，这帮助网络理解“同一相对位置但在不同分辨率下的表征之间的关系”（例如：某个像素在浅层是明显的小亮点，在深层被压缩成模糊小斑点），从而更好地定位和恢复超小目标的形态。论文的可视化（Fig.3）显示使用 SvAttn 后，薄细的血管、微小病变边界等能够被明显恢复。

一句话理解：SvAttn 是“跨层+按位置”的注意力，用多层同一相对位置的信息加权来找回被压缩掉的细节。

------

# 4) AssemFormer（图右上，绿色块：卷积 + Transformer 的拼装）

**结构细节**

- AssemFormer 的基本流水是：`Conv3×3 -> Conv1×1 -> Stack（reshape 成 patch 序列）-> Transformer ×2 -> Unstack（重构回 feature map）-> Conv1×1 -> Conv1×1`。论文说明：AssemFormer 通过 **stack / unstack** 操作把特征在卷积（局部）空间和 transformer（全局）序列空间之间转换，从而在同一模块里同时学习局部纹理和全局语义。论文里 AssemFormer 被放置在 MCBottleneck 内外（即在压缩前后都有可能），实验表明同时放内外效果最好。

**为什么要这么做**

- 单纯卷积擅长局部（边缘、角、纹理），但对跨图像长程语义有限；单纯 transformer 擅长全局依赖但缺少卷积的归纳偏置（对小样本/小结构不稳健）。AssemFormer 把两者“拼装（assemble）”到一起——先用卷积稳住局部特征，再用 transformer 建立补充的全局上下文，最后把结果再映回像素图（并通过跳跃/拼接保留细节）。论文的可视化（Fig.6）显示 AssemFormer 能把特征从粗到细逐层聚焦到小目标上。

一句话理解：AssemFormer 就是“卷积 + 堆叠成 patch + transformer + 回去”，把本地细节和全局上下文合在一个模块里，专门用于提升小目标的辨识与定位。

------

# 5) Decoder / 上采样（TConv）与最终组合

- 解码端用的 TConv3×3↓2（论文称为 TConv3×3↓2 的灰色块）不是单一反卷积，而是由 `1×1 conv -> 3×3 transposed strided conv -> 1×1 conv` 的组合，起到“可控上采样 + 通道变换”的作用。上采样后的特征会与 encoder 对应 stage 的特征做拼接/相加（论文表明用 **concatenate** 比单纯加法更有效）然后再经过 AssemFormer 等处理，最终由 ASPP 扩展多尺度感受野并输出预测。

一句话理解：解码用更稳健的上采样单元并保持丰富通道信息（拼接而不是简单相加），最后再用 ASPP 汇聚不同感受野的语义。

------

# 关键实现/超参数小结（论文里的实证细节）

- **MCAttn pooled sizes**：论文实验表明使用 pooled sizes (1,2,3)（对应 1×1、2×2、3×3）效果最好，能提升 mDice 和 sensitivity（见 Tab.VIII）。
- **AssemFormer 放置**：放在 MCBottleneck 的内外同时使用效果最好（论文 ablation，见 Tab.IX）。
- **整体提升**：在多个数据集（视网膜血管、皮肤病变、息肉、肝/肾肿瘤、细胞、精子）上，SvANet 在超小/小目标上均显著优于若干 SOTA 方法，说明该设计对“恢复/辨识超小目标”确实有效。

------

把整张网络想像成 **一个多层显微镜 + 人脑观察组合**：

- 显微镜（encoder）每过一级都会把图像放大（downsample），但同时有些“微小细节”会被压糊；
- 在每一级的“目镜里”放了一个专门的滤镜（MCBottleneck + MCAttn），这个滤镜会在不同放大倍率下随机看几眼（不同 pooling 大小），把不同尺度的信息都捕捉到；
- 同时，显微镜有条暗道（cross-scale guidance），把早期高分辨率的观察记录“裁好尺寸”后，稳稳地送到后面，用来提醒后面的观察者“这儿曾经有个小东西”；
- 再有一个智慧观察器（AssemFormer），它既能放大局部细节（卷积），又会把全图的语义关系整理成一张“笔记”（transformer），然后把笔记还原回图像空间，让网络既懂“这个像素是什么样子”，也懂“它和周围/整幅图像的关系”；
- 最后经过上采样和多尺度汇聚（ASPP），给出最终分割。

**SvANet 就是把“多尺度随机注意力（像不同倍率随机看几眼）”＋“跨层高分辨率提醒”＋“卷积和 transformer 的联合观察（本地细节 + 全局语义）”结合起来，专门为那些被下采样“吃掉”的超小目标做恢复与辨识。**
