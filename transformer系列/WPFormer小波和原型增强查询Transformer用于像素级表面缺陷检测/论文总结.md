### 1. 核心思想

该工作提出了一种用于像素级表面缺陷检测的**小波与原型增强的查询式 Transformer (WPFormer)**。其核心思想是构建一个**双域 Transformer 解码器 (D2T)**，旨在解决现有查询式分割方法中的两大瓶颈。具体而言，该解码器并行地在**频率域（小波域）**和**空间域**中增强查询（Query）与特征的交互：1) 在小波域，通过**小波增强交叉注意力 (WCA)** 模块，使查询能够关注富含边缘细节的高频信息，并自适应地抑制高频噪声；2) 在空间域，通过**原型引导交叉注意力 (PCA)** 模块，将特征自适应地聚类为“原型”，使查询能与高语义的原型交互，从而规避杂乱背景的干扰，且无需依赖（可能错误的）先验掩码。

### 2. 背景与动机

像素级表面缺陷检测 (SDD) 是智能制造中的关键环节，但面临巨大挑战，尤其是处理**弱缺陷**（尺寸小、形状细长、与背景高度相似）和**杂乱背景**时。

这项工作指出了现有方法的两大类问题：

1.  **FCN（全卷积网络）的局限性**：
    * 传统 FCN 及其变体（如图 1a）在预测层使用**静态卷积**来不加区分地分类所有像素特征。
    * 这种预测方式等同于使用一个“静态的、图像无关的”查询，它缺乏高级语义表征能力，导致在处理上述挑战性场景时（如弱缺陷和背景干扰），预测结果不佳，易丢失细节或产生误报。

2.  **现有查询式 Transformer 的局限性**：
    * 查询式方法（如图 1b）通过 Transformer 解码器动态学习一组查询（Query）与图像特征进行语义交互，在分割任务上表现优异。但将其直接应用于缺陷检测时，存在两个主要问题：
    * **问题一：仅限空间域交互**。现有方法（如图 1c）大多只在空间域计算查询-特征的交互。然而，弱缺陷（如细小裂纹）的辨识性特征往往体现在**高频域**（如边缘细节）。仅靠空间信息难以捕获这类缺陷。
    * **问题二：冗余信息干扰与先验依赖**。标准交叉注意力会计算查询与所有特征点（包括大量背景）的交互，导致查询的注意力被冗余背景信息稀释。为解决此问题，Mask2Former 和 PEM 等方法引入了“掩码注意力”，强制查询只关注前景区域。但这种方法**严重依赖一个高质量的掩码先验**，如果先验掩码本身就不完整或包含错误（例如，它没有预测出弱缺陷），这种错误将传导至后续解码过程，导致最终结果依然不佳。

因此，本文的动机是设计一种新型的查询式解码器，它既能利用频率域信息来增强弱缺陷的细节，又能（在不依赖掩码先验的前提下）有效编码关键缺陷信息、抑制空间背景冗余。

### 3. 主要贡献点

本文的主要贡献点可归纳为 3 项：

1. **提出 WPFormer 及其双域解码器 (D2T)**
   * 本文提出了一个**双域 Transformer 解码器 (D2T)**（如图 1d），它创新地将查询-特征的交互并行地在小波域和空间域进行。
   * **工作机理**：D2T 解码器层包含两个并行的注意力模块——WCA 和 PCA。输入的查询 Q 分别与特征 $F_i$ 在小波域（通过 WCA）和空间域（通过 PCA）进行交互。两个分支的输出被相加，然后送入一个自注意力（SA）层，得到最终更新的查询 $Q_{out}$。
   * **差异**：这种双域并行设计，使得网络能够同时从两个互补的维度（频率细节和空间语义）中提炼信息来更新查询，这与以往任何只在空间域操作的解码器都不同。

   ![结构图1](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251029204812.jpg)

2. **提出 WCA (小波增强交叉注意力)**

   * 该模块（如图 2a）被设计用于在频率域细化查询，使其更关注缺陷细节。

       ![结构图2](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251029204847.jpg)

   * **工作机理**：
       1.  使用哈尔小波 (DWT) 将特征 $F_i$ 分解为低频 $F_{fre}^l$ (即 $F_{LL}$) 和高频 $F_{fre}^h$ (即 $F_{LH}+F_{HL}+F_{HH}$)。
       2.  认识到 $F_{fre}^h$ 虽包含细节但也包含噪声，WCA 引入了一个**调制**步骤：它通过一个**多尺度上下文模块 (MSCM)**（如图 2c）学习全局和局部的信道权重，用以“抑制” $F_{fre}^h$ 中的噪声成分，得到 $F_{fre}^{h\prime}$。
       3.  将调制后的 $F_{fre}^{h\prime}$ 和 $F_{fre}^l$ 相加，得到一个既包含鲁棒低频结构又包含去噪高频细节的特征 $F_{fre}^\prime$。
       4.  最后，在标准的交叉注意力中，使用 $Q_{in}$ 作为 Query，使用 $F_{fre}^\prime$ 作为 Key 和 Value。
       
   * **差异**：与以往直接融合高低频特征的方法不同，WCA 关键在于对高频分量的**自适应调制**，这使得它能增强缺陷细节的同时抑制噪声，解决了频率域学习的常见痛点。

3. **提出 PCA (原型引导交叉注意力)**
   * 该模块（如图 2b）被设计用于在空间域细化查询，旨在不依赖掩码先验的情况下，让查询关注到关键的缺陷信息。
   * **工作机理**：
       1.  核心是一个**原型学习单元 (PLU)**。PLU 通过几个卷积层将空间特征 $F_i$（尺寸 $H \times W \times D$）自适应地“聚类”成 $M$ 个原型 $F_{pro}$（尺寸 $M \times D$，其中 $M=N$，即查询的数量）。
       2.  这个过程（如图 3 所示）将原始特征图（包含大量背景噪声）转化为了“原型激活图”，后者能更清晰地聚焦于缺陷区域。
       3.  在查询-原型交互时，它**不使用标准交叉注意力**，而是将 $F_{pro}$ 和 $Q_{in}$ 相加，并将和送入 MSCM 模块（与 WCA 共享设计），学习全局和局部的信道权重。
       4.  这些权重被用来调制 $Q_{in}$，实现查询的更新。
   * **差异**：PCA 与 PEM-CA 最大的不同在于：1) PCA 通过**自适应聚类**学习原型，完全摆脱了对掩码先验的依赖；而 PEM-CA 依赖掩码交叉注意力来获取原型。2) PCA 通过 MSCM 捕获**全局和局部**关系来更新查询，而 PEM-CA 仅捕获局部关系。

### 4. 方法细节

#### 4.1 整体架构 (Figure 2)

WPFormer 的整体架构是一个基于 PVTv2 Backbone 和 FPN 的编码器-解码器结构。

1.  **编码器 (Encoder)**：PVTv2 + FPN 提取多尺度特征 $\{F_i\}_{i=1}^4$。$F_1$ 是 $1/4$ 分辨率的高分辨率特征， $F_2 \sim F_4$ 是分辨率从高到低的多尺度特征。
2.  **查询初始化**：一组可学习的查询 $Q$ (N=16) 首先通过一个标准的 2 层 Transformer 解码器与高分辨率特征 $F_1$ 交互，进行初步更新。
3.  **D2T 解码器 (Decoder)**：
    * 更新后的 $Q$ 被送入一个 3 层的**双域 Transformer (D2T) 解码器**栈。
    * 这些解码器层**从低分辨率到高分辨率**（即 $F_4 \to F_3 \to F_2$）逐级处理特征。
    * 在每个 D2T 解码器块中（如图 2 右上角），输入查询 $Q_{in}$ 和该层对应的特征 $F_i$ 被**并行**送入 **WCA** 和 **PCA**。
    * $Q_{out} = SA(WCA(Q_{in}, F_i) + PCA(Q_{in}, F_i))$。
4.  **分割头 (SegHead)**：
    * 每个 D2T 解码器块的输出 $Q_{out}$ 都会被送入分割头，与高分辨率特征 $F_1$ 交互，生成一个掩码预测 $S_i$。
    * 最终的掩码是 $S_1 + S_2 + S_3$ 的总和。

#### 4.2 WCA (小波增强交叉注意力) 细节

WCA (图 2a) 的核心是利用小波分解的能力，并克服高频噪声的干扰。

* **理念**：弱缺陷的细节存在于高频，但原始高频分量（$F_{LH}, F_{HL}, F_{HH}$）包含大量噪声。因此，必须对高频分量进行“去噪”或“调制”后，才能安全地用于增强查询。
* **机制**：
    1.  **分解**：$F_i$ 通过 DWT 分解为 $F_{fre}^l$ (低频) 和 $F_{fre}^h$ (高频)。
    2.  **权重生成 (MSCM)**：将 $F_{fre}^h + F_{fre}^l$ 送入 MSCM 模块 (图 2c)。MSCM 并行计算两路权重：
        * 全局权重 $W_g^c$：通过 $GAP \to Linear \to \delta \to Linear$ 学习全局信道依赖。
        * 局部权重 $W_l^c$：通过 $Linear \to \delta \to Linear$ 学习局部（像素级）信道依赖。
    3.  **高频调制**：
        * 通过 $\sigma(W_g^c + W_l^c)$ 得到最终的注意力图。
        * $F_{fre}^{h\prime} = \sigma(W_g^c + W_l^c) \odot F_{fre}^h$
        * 这个逐元素乘法 $\odot$ 自适应地抑制了 $F_{fre}^h$ 中噪声所在的信道和空间位置。
    4.  **特征融合与注意力**：
        * $F_{fre}^\prime = F_{fre}^{h\prime} + F_{fre}^l$
        * $Q' = Norm(Q_{in} + Attention(Q_{in}, F_{fre}^\prime))$ (其中 $F_{fre}^\prime$ 同时作为 K 和 V)。

#### 4.3 PCA (原型引导交叉注意力) 细节

PCA (图 2b) 的核心是**原型学习单元 (PLU)**，它在不依赖掩码先验的前提下，实现了对空间信息的语义压缩。

* **理念**：缺陷检测中，绝大多数像素是背景。让查询与 $H \times W$ 个像素点逐一交互是低效且易受干扰的。PCA 的目标是先将 $H \times W$ 个特征点“聚类”成 $M$ 个语义原型（$M \ll H \times W$），然后让查询只与这 $M$ 个高语义的原型交互。
* **机制 (PLU)**：
    1.  **学习聚类中心**：输入 $F_i$ (尺寸 $H_i \times W_i \times D$) 通过两层卷积（$3 \times 3$ 和 $1 \times 1$）生成一个 $F_i^\prime$ (尺寸 $H_i \times W_i \times M$)。
    2.  **学习分配权重**：$F_i^\prime$ 被展平并通过 Softmax，得到 $F_i^w$ (尺寸 $H_iW_i \times M$)。这 $M$ 列中的每一列都是一个 Softmax 概率分布，代表了 $M$ 个原型中某一个原型对所有 $H_iW_i$ 个像素的“分配权重”。
    3.  **计算原型**：
        * $F_{pro} = Softmax(F_i^\prime)^T \otimes F_i$
        * 这里 $F_i^\prime$ 被展平，$F_i$ 也被展平 (尺寸 $H_iW_i \times D$)。
        * $Softmax(F_i^\prime)^T$ 的尺寸是 $M \times H_iW_i$。
        * 通过矩阵乘法，最终 $F_{pro}$ 的尺寸为 $M \times D$。
        * **总结**：$F_{pro}$ 是一个包含 $M$ 个原型的集合，每个原型都是 $F_i$ 中所有特征的加权平均，权重由网络自适应学习得到。如图 3 所示，这些原型能有效激活缺陷区域，忽略背景。

* **机制 (查询-原型交互)**：
    1.  PCA **不使用** $Q$ 和 $F_{pro}$ 之间的标准交叉注意力。
    2.  而是将 $Q_{in}$ (尺寸 $N \times D$，其中 $N=M$) 和 $F_{pro}$ (尺寸 $M \times D$) **逐元素相加**。
    3.  将 $F_{pro} + Q_{in}$ 的和送入 MSCM 模块，学习全局 $W_g^c$ 和局部 $W_l^c$ 权重。
    4.  使用这些权重对 $Q_{in}$ 进行调制和残差更新：
        * $Q' = Norm(\sigma(W_g^c + W_l^c) \odot Q_{in} + Q_{in})$
    * 这种设计通过 MSCM 捕获了原型和查询之间的全局及局部信道关系，从而有效地将原型的语义信息注入到查询中。

### 5. 即插即用模块的作用、适用场景和应用

本文的 WCA 和 PCA 模块具有很强的通用性，可以作为即插即用模块，被集成到其他基于 Transformer 的视觉架构中。

#### 5.1 WCA (小波增强交叉注意力)

* **作用**：
    WCA 是一个**频率域交叉注意力**模块。它替代了标准的空间交叉注意力，其核心作用是在查询和特征交互前，利用小波先验**增强高频细节**并**抑制高频噪声**。
* **适用场景**：
    适用于所有**细节纹理**至关重要，但又极易被**噪声**（或伪影）混淆的视觉任务。
* **具体应用**：
    1.  **医学图像分割**：如视网膜血管分割、CT/MRI 中的微小病灶（如微钙化点、早期肿瘤）检测，WCA 能帮助网络区分真实的精细结构和图像噪声。
    2.  **伪装/隐蔽目标检测 (COD)**：伪装目标往往与背景共享低频信息，仅在边缘等高频细节上有差异。WCA 能帮助查询聚焦于这些微弱的边界。
    3.  **图像恢复**：在 Transformer-based 的去噪、去模糊网络中，WCA 可以作为特征交互模块，帮助模型更好地区分信号的高频和噪声的高频。
    4.  **其他高精度检测**：如遥感图像中的细小道路/河流提取、微小裂痕检测等。

#### 5.2 PCA (原型引导交叉注意力)

* **作用**：
    PCA 是一种**空间注意力压缩**机制。它的核心是 PLU (原型学习单元)，PLU 可以作为一个独立的预处理层，插入到任何标准交叉注意力模块之前。它能将 $H \times W$ 的海量空间特征，自适应地压缩为 $M$ 个高语义的“原型”特征。
* **适用场景**：
    1.  **背景杂乱/冗余**：当图像背景复杂，包含大量干扰物时，PCA 能通过聚类，让查询只关注少数几个关键的语义原型，忽略背景。
    2.  **替代 Masked Attention**：当任务**缺乏可靠的先验掩码**时，PCA 提供了另一种（且无需先验的）过滤背景冗余的方案。
    3.  **计算效率提升**：在标准交叉注意力中，计算复杂度是 $O(N \cdot HW \cdot D)$。如果先用 PLU 将 $F_i$ 压缩为 $F_{pro}$，再让 $Q$ 与 $F_{pro}$ 交互，复杂度可降至 $O(N \cdot M \cdot D)$，在 $M \ll HW$ 时效率显著提升。
* **具体应用**：
    1.  **通用分割 (语义/实例/全景)**：可直接替换 Mask2Former 或 DETR 系列中的交叉注意力层，特别是在 COCO 等背景复杂的场景中，有望在不依赖掩码先验的情况下提升抗干扰能力。
    2.  **目标检测**：可用于 DETR-like 检测器的解码器中，压缩 FPN 特征为原型，加速收敛并减少背景误报。
    3.  **视频分析**：在视频分割中，可以将每一帧的空间特征压缩为一组紧凑的原型，用于时序建模。