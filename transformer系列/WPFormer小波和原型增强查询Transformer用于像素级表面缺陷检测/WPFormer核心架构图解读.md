# WPFormer 核心架构图解读

---

### 1. 动机图解 (Figure 1)：问题与方案

Figure 1 对比了四种不同的架构，清晰地阐明了本文的动机和高层解决方案。

![结构图1](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251029204812.jpg)

* **(a) FCN-based 方法**: 这是传统的全卷积网络范式，由编码器、解码器和最后的卷积层构成。这种方法的局限在于最后使用**静态卷积**进行预测，缺乏对图像内容的动态适应性。
* **(b) Query-based 方法**: 这是更先进的范式，它引入了一个 Transformer 解码器来动态更新一组（N个）可学习的查询（Queries）。这些查询与编码器-解码器提取的特征 M 进行交互，并最终通过权重 $W^p$ 聚合生成掩码。
* **(c) 空间 Transformer 解码器**: 这幅图揭示了**现有 Query-based 方法（如 b）的局限性**。其解码器层（Decoder Layer）只包含一个标准的交叉注意力（CA）模块，该模块让查询 Q 与空间特征 $F_i$ 直接交互。这在处理高频细节（如弱缺陷）和空间冗余（如杂乱背景）时能力不足。
* **(d) 提出的 D2T 解码器**: 这是本文的核心概念创新，即**双域 Transformer 解码器 (Dual-Domain Transformer Decoder)**。它没有采用单一的交互路径，而是设计了两个并行的分支：
    * **WCA (Wavelet-enhanced Cross-Attention)**：在频率域（小波域）进行交互。
    * **PCA (Prototype-guided Cross-Attention)**：在空间域（原型空间）进行交互。
    这两个分支的输出被相加，然后送入一个自注意力（SA）模块来更新查询 Q。

### 2. 整体网络架构 (Figure 2)：数据流

Figure 2 展示了所提出的 WPFormer 的完整架构和数据流。

![结构图2](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251029204847.jpg)

1.  **特征提取 (Encoder)**:
    * 输入图像首先通过 **Backbone (PVTv2)** 和 **FPN** 进行特征提取。
    * 这会产生一个高分辨率特征 $F_1$ 和三个多尺度特征 $F_2, F_3, F_4$。

2.  **查询初始化 (Query Initialization)**:
    * 一组可学习的 **N 个查询 (Q: N Queries)** 首先被送入一个标准的 **Transformer** 模块。
    * 在这个模块中，查询 Q 与高分辨率特征 $F_1$ 交互，进行初步的更新和初始化。

3.  **双域解码器栈 (D2T Decoder Stack)**:
    * 初始化后的查询 Q，被送入一个由 3 个 **D2T Decoder** 块组成的堆栈。
    * 这个堆栈**从低分辨率到高分辨率**（$F_4 \to F_3 \to F_2$）逐级处理多尺度特征。
    * 在**每个 D2T 解码器块**中（见图 2 右上角放大图），输入查询 $Q_{in}$ 和该层对应的特征 $F_i$ 被**并行**送入两个创新的注意力模块：
        1.  **WCA (Wavelet-enhanced Cross-Attention)**
        2.  **PCA (Prototype-guided Cross-Attention)**
    * WCA 和 PCA 的输出被**逐元素相加** (Element-wise Add)，然后送入一个**自注意力 (SA)** 模块，最终输出更新后的查询 $Q_{out}$。

4.  **掩码生成 (Mask Generation)**:
    * **每个 D2T 解码器**的输出查询 $Q_{out}$ 都会被送入一个 **SegHead** (分割头)。
    * SegHead 结合高分辨率特征 $F_1$ 来生成该阶段的分割掩码（$S_1, S_2, S_3$）。

### 3. 核心创新模块详解 (Figure 2, a, b, c)

Figure 2 的 (a), (b), (c) 子图详细拆解了 D2T 解码器的内部工作机制。

#### (a) WCA: 频率域交叉注意力

WCA 的目的是让查询 Q 与特征 $F_i$ 在**频率域**进行交互，以捕获高频细节。

1.  **小波分解**: 输入特征 $F_i$ 首先通过 **DWT** (离散小波变换) 模块，被分解为低频分量 $F_{fre}^l$ 和高频分量 $F_{fre}^h$。
2.  **高频调制 (核心)**:
    * 为了在增强细节的同时抑制高频噪声，WCA 并**不**直接使用 $F_{fre}^h$。
    * 它首先将 $F_{fre}^l$ 和 $F_{fre}^h$ 相加，并将结果送入 **MSCM** 模块 (图 2c) 以学习多尺度上下文权重。
    * MSCM 的输出权重会与原始的 $F_{fre}^h$ 进行**逐元素相乘** (Element-wise Product)，实现对高频信息的自适应调制（即去噪和增强）。
3.  **特征重组**: 调制后的高频特征 $F_{fre}^h$ 与低频特征 $F_{fre}^l$ **相加** (Element-wise Add)，得到最终用于注意力的特征。
4.  **交叉注意力**: 将重组后的特征 Reshape (重塑) 并送入标准的**交叉注意力 (Cross Attention)** 模块（作为 K 和 V），与输入的查询（作为 Q）进行交互。

#### (b) PCA: 原型引导交叉注意力

PCA 的目的是让查询 Q 与特征 $F_i$ 在**空间域**进行更高效、抗干扰的交互。

1.  **原型学习 (PLU)**:
    * PCA 的核心是 **PLU (Prototype Learning Unit)** 模块。
    * 它通过一个并行的卷积 (Conv) 和归一化 (Norm) 分支，学习一个 $H_i \times W_i \times M$ 的权重图。
    * 通过**矩阵乘法 (Matrix Mul.)**，PLU 将原始的 $F_i$ 特征（$H_iW_i \times D$）“聚类”或“压缩”为 **M 个原型**（$F_{pro}$，尺寸为 $M \times D$）。
2.  **查询-原型交互**:
    * 与 WCA 不同，PCA **不使用**标准的交叉注意力。
    * 它将学习到的 M 个原型 $F_{pro}$ 与 N 个输入查询 $Q_{in}$（其中 $M=N$）进行**逐元素相加**。
3.  **查询调制**:
    * 这个相加后的特征被送入 **MSCM** 模块 (图 2c) 以学习一组信道权重。
    * 这组权重被用来与**原始的 $Q_{in}$** 进行**逐元素相乘**，从而根据原型信息来调制查询。
    * 最后，通过一个残差连接（Element-wise Add）完成对查询的更新。

#### (c) MSCM: 多尺度上下文模块

MSCM 是一个被 WCA 和 PCA 共享的**信道注意力模块**，用于生成多尺度权重。

1.  **全局路径**: 输入特征通过 **GAP** (全局平均池化)，然后是两个 **Linear** (线性) 层，以捕获全局信道依赖。
2.  **局部路径**: 输入特征直接通过两个 **Linear** 层，以捕获局部的（像素级）信道依赖。
3.  **融合**: 两条路径的输出被相加，并通过一个 **Sigmoid** 激活函数，生成最终的注意力权重。

### 4. 图解总结

这组架构图清晰地展示了一个从问题到解决方案的完整逻辑链：

1.  **问题 (Fig 1c)**: 现有的 Transformer 解码器仅在**空间域**进行交叉注意力，这对于捕捉高频细节（如弱缺陷）和抑制背景噪声（空间冗余）是不够的。
2.  **方案 (Fig 1d)**: 提出 **D2T 解码器**，通过 **WCA** 和 **PCA** 两个并行分支，将交互扩展到**频率域**和**原型空间**。
3.  **WCA (Fig 2a) 的作用**: 解决了**高频细节**问题。它利用 DWT 提取高频信息，并通过 MSCM 进行智能调制，使查询能够关注到去噪后的缺陷边缘细节。
4.  **PCA (Fig 2b) 的作用**: 解决了**空间冗余**问题。它不让查询 Q 与 $H \times W$ 个像素点（包含大量背景）直接交互，而是先通过 PLU 将特征图自适应地聚类为 M 个有意义的“原型”，再让查询与这些高语义的原型交互，从而高效地聚焦于缺陷区域，抵抗背景干扰。